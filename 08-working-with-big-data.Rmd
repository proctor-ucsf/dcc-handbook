# Working with Big Data {#bigdata}


_Contributors: Eric Kim, Kunal Mishra and Jade Benjamin-Chung_

## Basics
A pitfall of working in R is that all objects are stored in memory - this makes it very difficult to work with datasets that are larger than 1-2 Gb for most standard computers. Here, we'll explore some alternatives to working with big data.

The Berkeley Statistical Computing Facility also has many good [training resources](http://statistics.berkeley.edu/computing/training).

## Using downsampled data
In studies with very large datasets, we save "downsampled" data that usually includes a 1% random sample stratified by any important variables, such as year or household id. This allows us to efficiently write and test our code without having to load in large, slow datasets that can cause RStudio to freeze. Be very careful to be sure which dataset you are working with and to label results output accordingly. 

## Unix {#big-data-unix}
Though bash is very commonly used for management of your file system (see Chapter \@ref(unix)), it is also a very capable at doing basic data manipulation with big data. At the core, since the data is stored on disk, you avoid having to overload memory when using bash commands as it will work with the files directly. By default, these commands will print the results to standard output (probably your terminal screen) and you can then redirect the results to other files on disk. These commands can also be chained via pipes (represented as `|`, similar to `%>%` in tidyverse). All of these have a list of arguments that can be passed in via flags (check the `man` page for more details on each).

| Command  | Description |
|----------|-------------|
| `head`/`tail` | Displays the first few or last few rows of a file |
| `cat` | Concatenates files and prints them |
| `sort` | Sorts the file |
| `cut` | Cuts out portions of each line and prints it |
| `grep` | Finds lines of a file that matches inputted patterns |
| `sed` | Find and replace |
| `awk` | Similar to `grep` and `sed` but with some extra programmatic functionality |
| `uniq` | Unifies repeated lines (combine with `sort` to get unique rows) |
| `wget` / `curl` | Downloads data/files from websites |

## SQL and `dbplyr`
SQL databases are relational databases that are a collection of _tables_ that consists of _fields_ or _attributes_, each containing a single _type_. If you use `dplyr` a lot, you will find that it is heavily inspired with a SQL flavor in mind. Formally, data gets loaded onto a database system and it is stored on disk. This alone makes working with data fast, but the real efficiency gain is the concept of indexing. If you are curious, most SQL databases implement their index with B trees or B+ trees, which allow for log time complexity for search operations in average and worst case scenarios while providing constant time complexity in best case scenario.

The basic structure of a SQL query is as follows:
```
SELECT [DISTINCT] (attributes)
FROM (table)
[WHERE (conditions)]
[GROUP BY (attributes) [HAVING (conditions)]]
[ORDER BY (attributes) [DESC]]
```

The equivalent `dplyr` command would look as such:
```
table %>%
  select(attributes) %>%     # distinct(attributes) for select distinct
  group_by(attributes) %>%   # 
  filter(conditions) %>%     # 
  arrange(attributes)        # arrange(desc(attributes)) for descending
```

There is ample support for connection to databases in R, and, in particular, there is the [`dbplyr`](https://dbplyr.tidyverse.org) package, which allows you to interface with the data with `dplyr` code instead of SQL code.

## `data.table` and `dtplyr`
It is often possible to load large datasets into memory in R, however computations will probably be very slow. One way around this is to use `data.table`. You will find that operations on data are much faster than base R or `dplyr` even though data is loaded into memory - this is because of clever programming in C as well as internally creating a _key_ (the SQL equivalent of an index) by default when loading in the data. You can improve on this even more by setting extra keys for variables you know you will be doing filter or join operations on. 

More recently from the tidyverse, is the implementation of [`dtplyr`](https://dtplyr.tidyverse.org), which allows for `dplyr` syntax on `data.table` objects.

An overview of the `dplyr` vs `data.table` debate can be found in [this stackoverflow post](https://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly/27840349#27840349) and all 3 answers are worth a read.

## `ff`, `bigmemory`, `biglm`
Sometimes, it may be impossible to load data into memory. Because of the overhead required, you can expect around twice as much memory needed as the size of the file on disk to just load in a sufficiently large dataset. One way to work around this is to keep the data on disk and instead create clever data structures that allow for natural interfacing with the data in an R session while mapping operations to the data on disk. Two packages that implement these ideas are `ff` and `bigmemory`.

We can now interface with the data while avoiding loading it into memory but we run into issues when we try to fit models on it. For an $n \times p$ dataset, linear regression has a time complexity of $O(np^2 + p^3)$ and a space complexity of $O(np + p^2)$ (this just means it will take a while and take up a lot of space for large $n$ and even moreso for large $p$). Clever solutions used in machine learning (think iterative algorithms stochastic gradient descent) can help us here. The idea is totake a smaller portion of our data, fit regression then update the coefficient based on another run of linear regression on another small portion of the data. For GLM models, this can be done with the `biglm` package which has integration with `ff` and `bigmemory`.

## Parallel computing
### Embarassingly Parallel Problems
Sometimes, we have to do something in a loop-like structure where each iteration may be independent of each other (think, simulations or bootstrap). These types of loops are referred to as [_embarassingly parallel_](https://www.google.com/search?client=safari&rls=en&q=embarassingly+parallel&ie=UTF-8&oe=UTF-8) problems (I did not make this up). Each iteration may take some time and every iteration thereafter must wait because the loop is essentially operation as a queue. This is where parallel computing comes into play: every computer these days come with at least 2 cores in the CPU. Each CPU core can operate independently so after some overhead, we can speed up our loop by the number of cores our computers have.

### Packages
In R, the popular packages are `parallel`, `foreach`, and `doParallel` (the backend that connects `foreach` and `parallel`). More modern parallel computing packages in R are `future` and `furrr` (inspired by `future purrr`, it allows for `purrr` like syntax using the `future` data structure from its namesake package). In Python, the `Dask` package has similar functionality to `future`. Note: the `parallel` package comes with a `detectCores` function, but I sometimes find that it is not accurate. On a mac, you can manually check the number of cores by going into About This Mac then System Report then checking the Total Number of Cores in the Hardware tab.

### GPU's
For most everyday tasks, CPU will be sufficient, but for large problems even an 8x speed boost (for a computer with 8 cores) might not be enough. This is where GPU's come into play. While CPU cores are good at complex operations, GPU cores are good at many small operations (think matrix multiplication). As GPU cores come in the hundreds for cheaper graphics cards and thousands for top end graphics cards, they are ideal for training machine learning models, particularly neural networks. However, as these GPU cores were intended for the graphics on our computers, we cannot access their computing power out of R or Python without some translation in between. Graphics manufacturers have been catching up to this market, and one of the most popular platforms for parallel computing on GPU's is Nvidia's [CUDA](https://developer.nvidia.com/cuda-zone) for use with Nvidia graphics cards. 

### The _MapReduce_ paradigm
The idea of the _MapReduce_ paradigm is that we can distribute the data across many nodes and try to do the computation on each piece of the data in each node. One benefit of this is that if our data is too large to fit on disk for a single machine, we can instead spread it across many then do our operation on parallel and aggregate it back together. We can formalize this paradigm into three steps

* Map: Split the data into sub-datasets and perform an operation on each entry in each sub-dataset thereby creating key-value pairs. 
* Shuffle: Merge the key-value pairs and sort them.
* Reduce: Apply an operation on the associated values for each key.

An excellent example is included at the bottom of this [link](https://www.journaldev.com/8848/mapreduce-algorithm-example). A similar paradigm that is implemented in the tidyverse is the [_split-apply-combine_](https://www.jstatsoft.org/article/view/v040i01) strategy.

The popular infrastructures for doing parallel computing with the MapReduce paradigm are Hadoop and Spark (think of Spark as an in-memory version of Hadoop). Spark can be interfaced with through Python via PySpark or R via SparkR (from Apache) or sparklyr (from RStudio), however note that Spark is natively implemented in Java and Scala so the overhead of serialization between R/Python to Java/Scala may be a time expensive operation.


## Optimal RStudio set up 

Using the following settings will help ensure a smooth experience when working with big data. In RStudio, go to the "Tools" menu, then select "Global Options". Under "General":

**Workspace**

- **Uncheck** Restore RData into workspace at startup 
- Save workspace to RData on exit -- choose **never**

**History**

- **Uncheck** Always save history


Unfortunately RStudio often gets slow and/or freezes after hours working with big datasets. Sometimes it is much more efficient to just use Terminal / gitbash to run code and make updates in git.

